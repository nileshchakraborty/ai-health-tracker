# AIDOC Environment Configuration
# Copy to .env and customize

# ═══════════════════════════════════════════════════════════════
# CORE (Required)
# ═══════════════════════════════════════════════════════════════

# AI Provider (simple setup - just need Ollama running)
AI_ADAPTER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2

# Database
DATABASE_ADAPTER=memory

# Server
PORT=3000
NEXT_PUBLIC_API_URL=http://localhost:3000

# ═══════════════════════════════════════════════════════════════
# OURA RING (Optional - for health data)
# ═══════════════════════════════════════════════════════════════

# Get token at: https://cloud.ouraring.com/personal-access-tokens
OURA_ACCESS_TOKEN=

# ═══════════════════════════════════════════════════════════════
# FEATURE FLAGS (Optional - advanced features)
# ═══════════════════════════════════════════════════════════════

# Set to 'true' to enable advanced features:
# ENABLE_LITELLM=true         # Multi-provider AI with fallbacks
# ENABLE_CIRCUIT_BREAKER=true # Resilience patterns
# ENABLE_GRPC=true            # gRPC server for mobile

# ═══════════════════════════════════════════════════════════════
# ADVANCED: LiteLLM (when ENABLE_LITELLM=true)
# ═══════════════════════════════════════════════════════════════

# Cloud API keys for fallbacks
# OPENAI_API_KEY=
# ANTHROPIC_API_KEY=

# LiteLLM config
# LITELLM_PRIMARY_MODEL=ollama/llama3.2
# LITELLM_FALLBACK_MODELS=openai/gpt-4o-mini,anthropic/claude-3-haiku
# LITELLM_CACHE_ENABLED=true

# ═══════════════════════════════════════════════════════════════
# ADVANCED: OAuth2 (production Oura integration)
# ═══════════════════════════════════════════════════════════════

# OURA_CLIENT_ID=
# OURA_CLIENT_SECRET=
# OURA_REDIRECT_URI=http://localhost:3000/api/oura/callback

# ═══════════════════════════════════════════════════════════════
# ADVANCED: gRPC (when ENABLE_GRPC=true)
# ═══════════════════════════════════════════════════════════════

# GRPC_PORT=50051
